{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\picar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn import feature_extraction, linear_model, model_selection, preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re # Regular expressions\n",
    "from wordcloud import STOPWORDS # Stopwords\n",
    "import string\n",
    "from nltk import FreqDist, word_tokenize, bigrams\n",
    "import nltk\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import subprocess\n",
    "import datetime as dt\n",
    "nltk.download('punkt')\n",
    "import category_encoders as ce\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"data/train.csv\")\n",
    "test_df = pd.read_csv(\"data/test.csv\")\n",
    "train_shape = train_df.shape\n",
    "test_shape = test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# id : Identifiant unique du tweet - integer\n",
    "# location : Endroit d'où à été posté le tweet\n",
    "# text : text of the tweet\n",
    "# keyword : name of the  \n",
    "#\n",
    "# target : label, bool. 1 if real disaster, 0 if not \n",
    "\n",
    "train_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape size is 7613 rows and 5 columns.\n",
      "In the train dataset, there are 61 items where we can't found any keywords.\n",
      "In the train dataset, there are 2533 items where we can't found any locations.\n"
     ]
    }
   ],
   "source": [
    "keyword = train_df.isnull().sum().keyword\n",
    "location = train_df.isnull().sum().location\n",
    "print('The shape size is {} rows and {} columns.'.format(train_shape[0], train_shape[1]))\n",
    "print(\"In the train dataset, there are {} items where we can't found any keywords.\".format(keyword))\n",
    "print(\"In the train dataset, there are {} items where we can't found any locations.\".format(location))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Must pass values for either `x` or `y`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-d2650d9df89c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mratio_1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0msns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcountplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_title\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Count of the targets in the train set.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"There is {}% of disasters in the train sample.\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mratio_1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Logiciel\\Progra\\Anaconda\\lib\\site-packages\\seaborn\\categorical.py\u001b[0m in \u001b[0;36mcountplot\u001b[1;34m(x, y, hue, data, order, hue_order, orient, color, palette, saturation, dodge, ax, **kwargs)\u001b[0m\n\u001b[0;32m   3546\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Cannot pass values for both `x` and `y`\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3547\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3548\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Must pass values for either `x` or `y`\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3549\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3550\u001b[0m     plotter = _BarPlotter(x, y, hue, data, order, hue_order,\n",
      "\u001b[1;31mTypeError\u001b[0m: Must pass values for either `x` or `y`"
     ]
    }
   ],
   "source": [
    "ratio_1 = train_df.target.value_counts()[1] / (train_df.target.value_counts()[0] + train_df.target.value_counts()[1])\n",
    "sns.countplot(X=train_df.target).set_title(\"Count of the targets in the train set.\")\n",
    "print(\"There is {}% of disasters in the train sample.\".format(ratio_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,6))\n",
    "sns.countplot(y=train_df.keyword, order=train_df.keyword.value_counts().iloc[:15].index)\n",
    "plt.title('Top 15 keywords')\n",
    "plt.show()\n",
    "\n",
    "print(\"There are {} unique keywords\".format(train_df.keyword.nunique()))\n",
    "\n",
    "kw_d = train_df[train_df.target==1].keyword.value_counts().head(10)\n",
    "kw_nd = train_df[train_df.target==0].keyword.value_counts().head(10)\n",
    "\n",
    "plt.figure(figsize=(13,5))\n",
    "plt.subplot(121)\n",
    "sns.barplot(kw_d, kw_d.index, color='c')\n",
    "plt.title('Top keywords for disaster tweets')\n",
    "plt.subplot(122)\n",
    "sns.barplot(kw_nd, kw_nd.index, color='y')\n",
    "plt.title('Top keywords for non-disaster tweets')\n",
    "plt.show()\n",
    "\n",
    "top_d = train_df.groupby('keyword').mean()['target'].sort_values(ascending=False).head(10)\n",
    "top_nd = train_df.groupby('keyword').mean()['target'].sort_values().head(10)\n",
    "\n",
    "    plt.figure(figsize=(13,5))\n",
    "plt.subplot(121)\n",
    "sns.barplot(top_d, top_d.index, color='pink')\n",
    "plt.title('Keywords with highest % of disaster tweets')\n",
    "plt.subplot(122)\n",
    "sns.barplot(top_nd, top_nd.index, color='yellow')\n",
    "plt.title('Keywords with lowest % of disaster tweets')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,6))\n",
    "sns.countplot(y=train_df.location, order=train_df.location.value_counts().iloc[:15].index)\n",
    "plt.title('Top 15 locations.')\n",
    "plt.show()\n",
    "\n",
    "raw_loc = train_df.location.value_counts()\n",
    "top_loc = list(raw_loc[raw_loc>=10].index)\n",
    "top_10_only = train_df[train_df.location.isin(top_loc)]\n",
    "\n",
    "print(\"There are {} unique locations. In these {} locations, only {} appears more than 10 times.\".format(train_df.location.nunique(), train_df.location.nunique(), len(top_10_only)))\n",
    "\n",
    "top_l = top_10_only.groupby('location').mean()['target'].sort_values(ascending=False)\n",
    "plt.figure(figsize=(14,6))\n",
    "sns.barplot(x=top_l.index, y=top_l)\n",
    "plt.axhline(np.mean(train_df.target))\n",
    "plt.xticks(rotation=80)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text) : \n",
    "    text = re.sub(r'https?://\\S+', '', text) # Remove link\n",
    "    text = re.sub(r'\\n',' ', text) # Remove line breaks\n",
    "    text = re.sub('\\s+', ' ', text).strip() # Remove leading, trailing, and extra spaces\n",
    "    return text\n",
    "\n",
    "def find_hashtags(tweet):\n",
    "    return \" \".join([match.group(0)[1:] for match in re.finditer(r\"#\\w+\", tweet)]) or 'no'\n",
    "\n",
    "def find_mentions(tweet):\n",
    "    return \" \".join([match.group(0)[1:] for match in re.finditer(r\"@\\w+\", tweet)]) or 'no'\n",
    "\n",
    "def find_links(tweet):\n",
    "    return \" \".join([match.group(0)[:] for match in re.finditer(r\"https?://\\S+\", tweet)]) or 'no'\n",
    "\n",
    "def process_text(df):\n",
    "    \n",
    "    df['text_clean'] = df['text'].apply(lambda x: clean_text(x))\n",
    "    df['hashtags'] = df['text'].apply(lambda x: find_hashtags(x))\n",
    "    df['mentions'] = df['text'].apply(lambda x: find_mentions(x))\n",
    "    df['links'] = df['text'].apply(lambda x: find_links(x))\n",
    "    # df['hashtags'].fillna(value='no', inplace=True)\n",
    "    # df['mentions'].fillna(value='no', inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_stat(df):\n",
    "    # Tweet length\n",
    "    df['text_len'] = df['text_clean'].apply(len)\n",
    "    # Word count\n",
    "    df['word_count'] = df[\"text_clean\"].apply(lambda x: len(str(x).split()))\n",
    "    # Stopword count\n",
    "    df['stop_word_count'] = df['text_clean'].apply(lambda x: len([w for w in str(x).lower().split() if w in STOPWORDS]))\n",
    "    # Punctuation count\n",
    "    df['punctuation_count'] = df['text_clean'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n",
    "    # Count of hashtags (#)\n",
    "    df['hashtag_count'] = df['hashtags'].apply(lambda x: len(str(x).split()))\n",
    "    # Count of mentions (@)\n",
    "    df['mention_count'] = df['mentions'].apply(lambda x: len(str(x).split()))\n",
    "    # Count of links\n",
    "    df['link_count'] = df['links'].apply(lambda x: len(str(x).split()))\n",
    "    # Count of uppercase letters\n",
    "    df['caps_count'] = df['text_clean'].apply(lambda x: sum(1 for c in str(x) if c.isupper()))\n",
    "    # Ratio of uppercase letters\n",
    "    df['caps_ratio'] = df['caps_count'] / df['text_len']\n",
    "    return df\n",
    "\n",
    "train_df = process_text(train_df)\n",
    "train_df = create_stat(train_df)\n",
    "\n",
    "train_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.corr()['target'].drop('target').sort_values() # Any correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(STOPWORDS)\n",
    "# Unigrams\n",
    "word_freq = FreqDist(w for w in word_tokenize(' '.join(train_df['text_clean']).lower()) if \n",
    "                     (w not in stopwords) & (w.isalpha()))\n",
    "df_word_freq = pd.DataFrame.from_dict(word_freq, orient='index', columns=['count'])\n",
    "top20w = df_word_freq.sort_values('count',ascending=False).head(20)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.barplot(top20w['count'], top20w.index)\n",
    "plt.title('Top 20 words')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,7))\n",
    "plt.subplot(121)\n",
    "freq_d = FreqDist(w for w in word_tokenize(' '.join(train_df.loc[train_df.target==1, 'text_clean']).lower()) if \n",
    "                     (w not in stopwords) & (w.isalpha()))\n",
    "df_d = pd.DataFrame.from_dict(freq_d, orient='index', columns=['count'])\n",
    "top20_d = df_d.sort_values('count',ascending=False).head(20)\n",
    "sns.barplot(top20_d['count'], top20_d.index, color='c')\n",
    "plt.title('Top words in disaster tweets')\n",
    "plt.subplot(122)\n",
    "freq_nd = FreqDist(w for w in word_tokenize(' '.join(train_df.loc[train_df.target==0, 'text_clean']).lower()) if \n",
    "                     (w not in stopwords) & (w.isalpha()))\n",
    "df_nd = pd.DataFrame.from_dict(freq_nd, orient='index', columns=['count'])\n",
    "top20_nd = df_nd.sort_values('count',ascending=False).head(20)\n",
    "sns.barplot(top20_nd['count'], top20_nd.index, color='y')\n",
    "plt.title('Top words in non-disaster tweets')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(16,7))\n",
    "plt.subplot(121)\n",
    "bigram_d = list(bigrams([w for w in word_tokenize(' '.join(train_df.loc[train_df.target==1, 'text_clean']).lower()) if \n",
    "              (w not in stopwords) & (w.isalpha())]))\n",
    "d_fq = FreqDist(bg for bg in bigram_d)\n",
    "bgdf_d = pd.DataFrame.from_dict(d_fq, orient='index', columns=['count'])\n",
    "bgdf_d.index = bgdf_d.index.map(lambda x: ' '.join(x))\n",
    "bgdf_d = bgdf_d.sort_values('count',ascending=False)\n",
    "sns.barplot(bgdf_d.head(20)['count'], bgdf_d.index[:20], color='pink')\n",
    "plt.title('Top bigrams in disaster tweets')\n",
    "plt.subplot(122)\n",
    "bigram_nd = list(bigrams([w for w in word_tokenize(' '.join(train_df.loc[train_df.target==0, 'text_clean']).lower()) if \n",
    "              (w not in stopwords) & (w.isalpha())]))\n",
    "nd_fq = FreqDist(bg for bg in bigram_nd)\n",
    "bgdf_nd = pd.DataFrame.from_dict(nd_fq, orient='index', columns=['count'])\n",
    "bgdf_nd.index = bgdf_nd.index.map(lambda x: ' '.join(x))\n",
    "bgdf_nd = bgdf_nd.sort_values('count',ascending=False)\n",
    "sns.barplot(bgdf_nd.head(20)['count'], bgdf_nd.index[:20], color='yellow')\n",
    "plt.title('Top bigrams in non-disaster tweets')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "columns overlap but no suffix specified: Index(['keyword_target', 'location_target'], dtype='object')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-7338d80d7c9f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mencoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'target'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mtrain_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_suffix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'_target'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mvec_links\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmin_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0manalyzer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'word'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtoken_pattern\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mr'https?://\\S+'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Only include those >=5 occurrences\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mlink_vec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvec_links\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'links'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Logiciel\\Progra\\Anaconda\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mjoin\u001b[1;34m(self, other, on, how, lsuffix, rsuffix, sort)\u001b[0m\n\u001b[0;32m   6813\u001b[0m         \u001b[1;31m# For SparseDataFrame's benefit\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6814\u001b[0m         return self._join_compat(other, on=on, how=how, lsuffix=lsuffix,\n\u001b[1;32m-> 6815\u001b[1;33m                                  rsuffix=rsuffix, sort=sort)\n\u001b[0m\u001b[0;32m   6816\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6817\u001b[0m     def _join_compat(self, other, on=None, how='left', lsuffix='', rsuffix='',\n",
      "\u001b[1;32mD:\\Logiciel\\Progra\\Anaconda\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_join_compat\u001b[1;34m(self, other, on, how, lsuffix, rsuffix, sort)\u001b[0m\n\u001b[0;32m   6828\u001b[0m             return merge(self, other, left_on=on, how=how,\n\u001b[0;32m   6829\u001b[0m                          \u001b[0mleft_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mon\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mright_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6830\u001b[1;33m                          suffixes=(lsuffix, rsuffix), sort=sort)\n\u001b[0m\u001b[0;32m   6831\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6832\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mon\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Logiciel\\Progra\\Anaconda\\lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36mmerge\u001b[1;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m     46\u001b[0m                          \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindicator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mindicator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m                          validate=validate)\n\u001b[1;32m---> 48\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Logiciel\\Progra\\Anaconda\\lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36mget_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    550\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    551\u001b[0m         llabels, rlabels = items_overlap_with_suffix(ldata.items, lsuf,\n\u001b[1;32m--> 552\u001b[1;33m                                                      rdata.items, rsuf)\n\u001b[0m\u001b[0;32m    553\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    554\u001b[0m         \u001b[0mlindexers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mleft_indexer\u001b[0m\u001b[1;33m}\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mleft_indexer\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Logiciel\\Progra\\Anaconda\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36mitems_overlap_with_suffix\u001b[1;34m(left, lsuffix, right, rsuffix)\u001b[0m\n\u001b[0;32m   1970\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mlsuffix\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mrsuffix\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1971\u001b[0m             raise ValueError('columns overlap but no suffix specified: '\n\u001b[1;32m-> 1972\u001b[1;33m                              '{rename}'.format(rename=to_rename))\n\u001b[0m\u001b[0;32m   1973\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1974\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mlrenamer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: columns overlap but no suffix specified: Index(['keyword_target', 'location_target'], dtype='object')"
     ]
    }
   ],
   "source": [
    "features = ['keyword', 'location']\n",
    "encoder = ce.TargetEncoder(cols=features)\n",
    "encoder.fit(train_df[features],train_df['target'])\n",
    "\n",
    "train_df = train_df.join(encoder.transform(train_df[features]).add_suffix('_target'))\n",
    "\n",
    "# Links\n",
    "vec_links = CountVectorizer(min_df = 5, analyzer = 'word', token_pattern = r'https?://\\S+') # Only include those >=5 occurrences\n",
    "link_vec = vec_links.fit_transform(train_df['links'])\n",
    "X_train_link = pd.DataFrame(link_vec.toarray(), columns=vec_links.get_feature_names())\n",
    "\n",
    "# Mentions\n",
    "vec_men = CountVectorizer(min_df = 5)\n",
    "men_vec = vec_men.fit_transform(train_df['mentions'])\n",
    "X_train_men = pd.DataFrame(men_vec.toarray(), columns=vec_men.get_feature_names())\n",
    "\n",
    "# Hashtags\n",
    "vec_hash = CountVectorizer(min_df = 5)\n",
    "hash_vec = vec_hash.fit_transform(train_df['hashtags'])\n",
    "X_train_hash = pd.DataFrame(hash_vec.toarray(), columns=vec_hash.get_feature_names())\n",
    "print (X_train_link.shape, X_train_men.shape, X_train_hash.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File data/my_submission.csv successfully generated.\n",
      "\n",
      "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.15 / client 1.5.13)\n",
      "Successfully submitted to Natural Language Processing with Disaster Tweets\n",
      "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.15 / client 1.5.13)\n",
      "fileName               date                 description                          status    publicScore  privateScore  \n",
      "---------------------  -------------------  -----------------------------------  --------  -----------  ------------  \n",
      "my_submission.csv      2023-08-23 20:05:19  2023-08-23 21:05:16: New submission  complete  0.00000                    \n",
      "sample_submission.csv  2023-08-23 19:58:34  test                                 complete  0.57033                    \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def generate_submission_file_based_on_prediction(result, source_file_path=\"data/sample_submission.csv\"):\n",
    "    df = pd.read_csv(\n",
    "        filepath_or_buffer=source_file_path,\n",
    "        sep=\",\",\n",
    "    ).set_index(\"PassengerId\")\n",
    "    print(f\"Entry: {df.shape}.\")\n",
    "    merge_df = pd.merge(\n",
    "        left=df,\n",
    "        right=result,\n",
    "        how=\"left\",\n",
    "        left_index=True,\n",
    "        right_index=True,\n",
    "    )\n",
    "    print(f\"Output: {merge_df.shape}.\")\n",
    "    if df.shape[0] != merge_df.shape[0]:\n",
    "        raise ValueError(f\"Should be same size.\")\n",
    "    merge_df = merge_df[[\"Transported_x\", \"Transported_y\"]]\n",
    "    merge_df = merge_df.reset_index().rename(columns={\"Transported_y\": \"Transported\"}).drop(columns=[\"Transported_x\"])\n",
    "    merge_df.to_csv(\"data/my_submission.csv\", sep=\",\", index=False)\n",
    "    \n",
    "def generate_random_submission(source_file_path=\"data/sample_submission.csv\"):\n",
    "    df = pd.read_csv(\n",
    "        filepath_or_buffer=source_file_path,\n",
    "        sep=\",\",\n",
    "    )\n",
    "    df[\"target\"] = df[\"target\"].apply(lambda x: random.uniform(0.0, 1.0))\n",
    "    print(f\"File data/my_submission.csv successfully generated.\\n\")\n",
    "    df.to_csv(\"data/my_submission.csv\", sep=\",\", index=False)\n",
    "    \n",
    "def submit_submission(submission_file=\"data/my_submission.csv\"):\n",
    "    with open(\"kaggle.json\") as credential:\n",
    "        json_credential = json.loads(credential.read())\n",
    "        os.environ[\"KAGGLE_USERNAME\"] = json_credential[\"username\"]\n",
    "        os.environ[\"KAGGLE_KEY\"] = json_credential[\"key\"]\n",
    "    result = subprocess.check_output(\n",
    "        [\n",
    "            \"kaggle\",\n",
    "            \"competitions\",\n",
    "            \"submit\",\n",
    "            \"nlp-getting-started\",\n",
    "            \"-f\",\n",
    "            submission_file,\n",
    "            \"-m\",\n",
    "            f\"{dt.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}: New submission\",\n",
    "        ]\n",
    "    ).decode(\"utf-8\")\n",
    "    print(result)\n",
    "\n",
    "def get_latest_score(team_id=\"10059555\"):\n",
    "    with open(\"kaggle.json\") as credential:\n",
    "        json_credential = json.loads(credential.read())\n",
    "        os.environ[\"KAGGLE_USERNAME\"] = json_credential[\"username\"]\n",
    "        os.environ[\"KAGGLE_KEY\"] = json_credential[\"key\"]\n",
    "        os.environ[\"KAGGLE_TEAM_ID\"] = team_id\n",
    "    result = subprocess.check_output([\"kaggle\", \"competitions\", \"submissions\", \"nlp-getting-started\"]).decode(\"utf-8\")\n",
    "    print(result)\n",
    "\n",
    "generate_random_submission()\n",
    "submit_submission()\n",
    "get_latest_score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
